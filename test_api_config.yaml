model:
  size: small
data:
  train_data: ./data/tokenized/train.npy
  val_data: ./data/tokenized/val.npy
training:
  num_epochs: 10
  batch_size: 16
  learning_rate: 0.0003
  weight_decay: 0.01
  grad_clip: 1.0
  warmup_steps: 100
optimizer:
  betas:
  - 0.9
  - 0.95
  eps: 1.0e-08
checkpointing:
  checkpoint_dir: ./model/checkpoints
  save_interval: 1
  keep_last_n: 3
logging:
  log_dir: ./logs
  eval_interval: 100
  log_interval: 10
device:
  device: cuda
  seed: 42
advanced:
  max_steps: -1
  resume_from: null
  quick_test: false
