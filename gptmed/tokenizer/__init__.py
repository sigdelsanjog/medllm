"""
Tokenizer module for MedLLM
"""

from llm_med.tokenizer.train_tokenizer import train_sentencepiece_tokenizer

__all__ = ["train_sentencepiece_tokenizer"]
